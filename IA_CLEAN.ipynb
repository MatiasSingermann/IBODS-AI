{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatiasSingermann/IBODS-AI/blob/main/IA_CLEAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD2VImwGmGJ8"
      },
      "source": [
        "## Importacion e instalacion de librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B9jQTvYlmCAn",
        "outputId": "ab17a69d-a3a0-4a74-8876-16cf608eb60f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting roboflow\n",
            "  Downloading roboflow-0.2.15.tar.gz (18 kB)\n",
            "Collecting certifi==2021.5.30\n",
            "  Downloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 13.1 MB/s \n",
            "\u001b[?25hCollecting chardet==4.0.0\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 69.1 MB/s \n",
            "\u001b[?25hCollecting cycler==0.10.0\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.7/dist-packages (from roboflow) (0.7)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.10)\n",
            "Collecting kiwisolver==1.3.1\n",
            "  Downloading kiwisolver-1.3.1-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 64.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from roboflow) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.21.6)\n",
            "Requirement already satisfied: opencv-python-headless>=4.5.1.48 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.6.0.66)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from roboflow) (7.1.2)\n",
            "Collecting pyparsing==2.4.7\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-0.21.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.23.0)\n",
            "Collecting requests_toolbelt\n",
            "  Downloading requests_toolbelt-0.10.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.15.0)\n",
            "Collecting urllib3==1.26.6\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 92.2 MB/s \n",
            "\u001b[?25hCollecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.64.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.7/dist-packages (from roboflow) (6.0)\n",
            "Collecting requests\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->roboflow) (2.1.1)\n",
            "Building wheels for collected packages: roboflow, wget\n",
            "  Building wheel for roboflow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for roboflow: filename=roboflow-0.2.15-py3-none-any.whl size=24936 sha256=87cf14dce4ffb5001a96ecf996e522891f661d77c9aa31c16320ad55574308e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/e0/a3/fe183fffc3972cb9264e800763d4bbfff80d5309137c410217\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=62eeeff6836906c421147a14abae93743a7c2d1f7b97c9d688fd364bcba3099c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built roboflow wget\n",
            "Installing collected packages: urllib3, certifi, requests, pyparsing, kiwisolver, cycler, wget, requests-toolbelt, python-dotenv, chardet, roboflow\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2022.9.24\n",
            "    Uninstalling certifi-2022.9.24:\n",
            "      Successfully uninstalled certifi-2022.9.24\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.9\n",
            "    Uninstalling pyparsing-3.0.9:\n",
            "      Successfully uninstalled pyparsing-3.0.9\n",
            "  Attempting uninstall: kiwisolver\n",
            "    Found existing installation: kiwisolver 1.4.4\n",
            "    Uninstalling kiwisolver-1.4.4:\n",
            "      Successfully uninstalled kiwisolver-1.4.4\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.11.0\n",
            "    Uninstalling cycler-0.11.0:\n",
            "      Successfully uninstalled cycler-0.11.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 3.0.4\n",
            "    Uninstalling chardet-3.0.4:\n",
            "      Successfully uninstalled chardet-3.0.4\n",
            "Successfully installed certifi-2021.5.30 chardet-4.0.0 cycler-0.10.0 kiwisolver-1.3.1 pyparsing-2.4.7 python-dotenv-0.21.0 requests-2.28.1 requests-toolbelt-0.10.0 roboflow-0.2.15 urllib3-1.26.6 wget-3.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "cycler",
                  "kiwisolver",
                  "pyparsing"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8COx3UfJmOeO",
        "outputId": "1d4c1bfd-f7ff-4ab3-bbd0-73d025302cfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tflite-model-maker\n",
            "  Downloading tflite_model_maker-0.4.2-py3-none-any.whl (577 kB)\n",
            "\u001b[K     |████████████████████████████████| 577 kB 13.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker) (6.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker) (22.9.24)\n",
            "Collecting tensorflow-model-optimization>=0.5\n",
            "  Downloading tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n",
            "\u001b[K     |████████████████████████████████| 238 kB 71.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: librosa==0.8.1 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker) (0.8.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker) (1.15.0)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 78.2 MB/s \n",
            "\u001b[?25hCollecting tensorflowjs<3.19.0,>=2.4.0\n",
            "  Downloading tensorflowjs-3.18.0-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub<0.13,>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker) (0.12.0)\n",
            "Requirement already satisfied: tensorflow>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker) (2.8.2+zzzcolab20220929150707)\n",
            "Collecting scann==1.2.6\n",
            "  Downloading scann-1.2.6-cp37-cp37m-manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 26.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython>=0.29.13 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker) (0.29.32)\n",
            "Collecting numba==0.53\n",
            "  Downloading numba-0.53.0-cp37-cp37m-manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 55.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.6.1 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker) (4.9.1)\n",
            "Requirement already satisfied: matplotlib<3.5.0,>=3.0.3 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker) (3.2.2)\n",
            "Requirement already satisfied: absl-py>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker) (1.2.0)\n",
            "Collecting tensorflow-addons>=0.11.2\n",
            "  Downloading tensorflow_addons-0.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 58.9 MB/s \n",
            "\u001b[?25hCollecting tf-models-official==2.3.0\n",
            "  Downloading tf_models_official-2.3.0-py2.py3-none-any.whl (840 kB)\n",
            "\u001b[K     |████████████████████████████████| 840 kB 51.5 MB/s \n",
            "\u001b[?25hCollecting fire>=0.3.1\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 6.8 MB/s \n",
            "\u001b[?25hCollecting neural-structured-learning>=1.3.1\n",
            "  Downloading neural_structured_learning-1.4.0-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 81.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-datasets>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker) (4.6.0)\n",
            "Requirement already satisfied: pillow>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker) (1.21.6)\n",
            "Collecting sentencepiece>=0.1.91\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 43.1 MB/s \n",
            "\u001b[?25hCollecting tflite-support>=0.4.2\n",
            "  Downloading tflite_support-0.4.2-cp37-cp37m-manylinux2014_x86_64.whl (60.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 60.2 MB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker) (1.0.2)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker) (4.4.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker) (3.0.0)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker) (0.11.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker) (0.4.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker) (1.7.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker) (21.3)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker) (1.6.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba==0.53->tflite-model-maker) (57.4.0)\n",
            "Collecting llvmlite<0.37,>=0.36.0rc1\n",
            "  Downloading llvmlite-0.36.0-cp37-cp37m-manylinux2010_x86_64.whl (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0->tflite-model-maker) (1.5.12)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0->tflite-model-maker) (1.12.11)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0->tflite-model-maker) (0.5.0)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Collecting tf-slim>=1.1.0\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 73.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-bigquery>=0.31.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0->tflite-model-maker) (1.21.0)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0->tflite-model-maker) (1.3.5)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0->tflite-model-maker) (4.6.0.66)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0->tflite-model-maker) (5.4.8)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
            "\u001b[K     |████████████████████████████████| 99 kB 11.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire>=0.3.1->tflite-model-maker) (2.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker) (3.0.1)\n",
            "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker) (1.31.6)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker) (0.17.4)\n",
            "Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker) (1.35.0)\n",
            "Requirement already satisfied: protobuf<4.0.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker) (3.17.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker) (1.56.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker) (2022.4)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker) (2.28.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker) (4.2.4)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.3.0->tflite-model-maker) (0.4.1)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.3.0->tflite-model-maker) (1.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official==2.3.0->tflite-model-maker) (4.64.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official==2.3.0->tflite-model-maker) (6.1.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official==2.3.0->tflite-model-maker) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official==2.3.0->tflite-model-maker) (2021.5.30)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.5.0,>=3.0.3->tflite-model-maker) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.5.0,>=3.0.3->tflite-model-maker) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.5.0,>=3.0.3->tflite-model-maker) (2.4.7)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from neural-structured-learning>=1.3.1->tflite-model-maker) (22.1.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.1->tflite-model-maker) (1.4.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker) (0.4.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker) (2.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa==0.8.1->tflite-model-maker) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa==0.8.1->tflite-model-maker) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.8.1->tflite-model-maker) (2.21)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (4.1.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (0.27.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (1.1.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (3.1.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (1.14.1)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (2.8.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (2.8.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (14.0.6)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (1.49.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.6.0->tflite-model-maker) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.6.0->tflite-model-maker) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker) (1.0.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker) (3.8.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker) (3.2.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons>=0.11.2->tflite-model-maker) (2.7.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (5.9.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (0.8.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (0.10.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (2.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (0.3.5.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (1.10.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.5->tflite-model-maker) (0.1.7)\n",
            "Collecting packaging>=20.0\n",
            "  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 7.3 MB/s \n",
            "\u001b[?25hCollecting protobuf<4.0.0dev,>=3.12.0\n",
            "  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 87.7 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.6.0\n",
            "  Downloading pybind11-2.10.0-py3-none-any.whl (213 kB)\n",
            "\u001b[K     |████████████████████████████████| 213 kB 81.6 MB/s \n",
            "\u001b[?25hCollecting sounddevice>=0.4.4\n",
            "  Downloading sounddevice-0.4.5-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.3.0->tflite-model-maker) (1.3)\n",
            "Building wheels for collected packages: fire, py-cpuinfo\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=e6ea5543453d8bdb50c02189ceb0e9223bb30ae82a187e03f1d4a8bd1ad963b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22257 sha256=c3931c98a8e12bd10d80076c1d01a394f74ec3f05e61cb41c0c3549a0f054e39\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
            "Successfully built fire py-cpuinfo\n",
            "Installing collected packages: urllib3, protobuf, packaging, llvmlite, numba, tf-slim, tensorflow-model-optimization, tensorflow-addons, sounddevice, sentencepiece, pybind11, py-cpuinfo, dataclasses, tflite-support, tf-models-official, tensorflowjs, scann, neural-structured-learning, fire, tflite-model-maker\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.6\n",
            "    Uninstalling urllib3-1.26.6:\n",
            "      Successfully uninstalled urllib3-1.26.6\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 21.3\n",
            "    Uninstalling packaging-21.3:\n",
            "      Successfully uninstalled packaging-21.3\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.39.1\n",
            "    Uninstalling llvmlite-0.39.1:\n",
            "      Successfully uninstalled llvmlite-0.39.1\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.56.2\n",
            "    Uninstalling numba-0.56.2:\n",
            "      Successfully uninstalled numba-0.56.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "roboflow 0.2.15 requires urllib3==1.26.6, but you have urllib3 1.25.11 which is incompatible.\u001b[0m\n",
            "Successfully installed dataclasses-0.6 fire-0.4.0 llvmlite-0.36.0 neural-structured-learning-1.4.0 numba-0.53.0 packaging-20.9 protobuf-3.19.6 py-cpuinfo-8.0.0 pybind11-2.10.0 scann-1.2.6 sentencepiece-0.1.97 sounddevice-0.4.5 tensorflow-addons-0.18.0 tensorflow-model-optimization-0.7.3 tensorflowjs-3.18.0 tf-models-official-2.3.0 tf-slim-1.1.0 tflite-model-maker-0.4.2 tflite-support-0.4.2 urllib3-1.25.11\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install tflite-model-maker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqB_uiCqmRhV"
      },
      "source": [
        "Importo `Tensorflow` y varios de sus **modulos**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tXV1oBSmmR5n"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Input\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tflite_model_maker import object_detector\n",
        "import tensorflow_hub as hub\n",
        "from tflite_model_maker.config import QuantizationConfig\n",
        "from tflite_support.task import vision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79MykpSfmaXW"
      },
      "source": [
        "Importo varias `librerias` que me permiten trabajar con **tensores**, y **operaciones matematicas** necesrias. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KZOSDS6tma-E"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_rKhmWmmg8Z"
      },
      "source": [
        "Importo `librerias` para **administrar archivos** y seleccionar de manera **random** alguno de ellos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Gc5A4SDhmkMH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq8dFtRPmn9T"
      },
      "source": [
        "Importo `librerias` que me permiten trabajar con las **imagenes** del dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cIJu9w4UmoDX"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.image import imread\n",
        "from itertools import product\n",
        "from skimage import draw, transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_yTfBIrmtq_"
      },
      "source": [
        "Importo `Roboflow` para acceder al dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "g2HBu838mznI"
      },
      "outputs": [],
      "source": [
        "from roboflow import Roboflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbD44He-m8MI"
      },
      "source": [
        "## Importacion del **Dataset** y sus **labels**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL2X8Gp9nWZI"
      },
      "source": [
        "Me conecto a mi cuenta de Drive para agarrar ciertos archivos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvjknquxnUjH",
        "outputId": "b24e9b78-3185-4620-9851-e3a4344483f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-X3KZaSna4Y"
      },
      "source": [
        "Importo el dataset desde Roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2tfSwUanD40",
        "outputId": "78e4ad6e-6557-474d-a5f2-00fb16b4b0d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in IBODS_LBL-1 to voc: 100% [157029367 / 157029367] bytes\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Dataset Version Zip to IBODS_LBL-1 in voc:: 100%|██████████| 7031/7031 [00:01<00:00, 5271.84it/s]\n"
          ]
        }
      ],
      "source": [
        "rf = Roboflow(api_key=\"41l2d60Eq7cWhUOVgPxd\")\n",
        "project = rf.workspace(\"ibods\").project(\"ibods_lbl\")\n",
        "dataset = project.version(1).download(\"voc\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zij8UDnCnjso"
      },
      "source": [
        "Asigno la **ruta del archivo** de las anotaciones a una variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "72ozUQ5inoSs"
      },
      "outputs": [],
      "source": [
        "path_lbl = '/content/gdrive/MyDrive/Proyecto 4to/I.A/Anotaciones/labels.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJE5EpcdnrZw"
      },
      "source": [
        "Abro el **archivo de anotaciones** y lo *recorro linea por linea*, *eliminando los espacios* y metiendo los `strings` en una `lista`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIUF-rPRn0Bm",
        "outputId": "e9135416-1762-4480-cb39-b87d566b3334"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['cordon',\n",
              " 'autos',\n",
              " 'personas',\n",
              " 'cruces',\n",
              " 'pozos',\n",
              " 'parar',\n",
              " 'cruzar',\n",
              " 'bicicleta',\n",
              " 'moto',\n",
              " 'escalones']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open(path_lbl) as f:\n",
        "   labels = [line.strip() for line in f.readlines()]\n",
        "labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbYsoSkhn1-R"
      },
      "source": [
        "Usando el **arhcivo de labels**, le asigno un *valor numerico* para que la `IA` pueda interpretar las anotaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPXmSf4Un4Kd",
        "outputId": "e7ae8364-baea-45bc-eb89-fd8d80604a7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'cordon',\n",
              " 1: 'autos',\n",
              " 2: 'personas',\n",
              " 3: 'cruces',\n",
              " 4: 'pozos',\n",
              " 5: 'parar',\n",
              " 6: 'cruzar',\n",
              " 7: 'bicicleta',\n",
              " 8: 'moto',\n",
              " 9: 'escalones'}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "assigned_labels = { i : labels[i] for i in range(0, len(labels) ) }\n",
        "assigned_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDDgePjQoCCI"
      },
      "source": [
        "## Funciones Utiles y configuracion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Z3DtEabhoCjD"
      },
      "outputs": [],
      "source": [
        "CLASSES = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Gsj3FE2ioGqt"
      },
      "outputs": [],
      "source": [
        "COLORS = np.random.randint(0, 255, size=(len(CLASSES), 3), dtype=np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "rXD6vgJSoZpC"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_path, input_size):\n",
        "  ''' Preprocess the input image to feed to the TFLite model\n",
        "  '''\n",
        "  img = tf.io.read_file(image_path)\n",
        "  img = tf.io.decode_image(img, channels=3)\n",
        "  img = tf.image.convert_image_dtype(img, tf.uint8)\n",
        "  original_image = img\n",
        "  resized_img = tf.image.resize(img, input_size)\n",
        "  resized_img = resized_img[tf.newaxis, :]\n",
        "  resized_img = tf.cast(resized_img, dtype=tf.uint8)\n",
        "  return resized_img, original_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Ghxh4WZ-oaLH"
      },
      "outputs": [],
      "source": [
        "def detect_objects(interpreter, image, threshold):\n",
        "  ''' Returns a list of detection results, each a dictionary of object info.\n",
        "  '''\n",
        "\n",
        "  signature_fn = interpreter.get_signature_runner()\n",
        "\n",
        "  # Feed the input image to the model\n",
        "  output = signature_fn(images=image)\n",
        "\n",
        "  # Get all outputs from the model\n",
        "  count = int(np.squeeze(output['output_0']))\n",
        "  scores = np.squeeze(output['output_1'])\n",
        "  classes = np.squeeze(output['output_2'])\n",
        "  boxes = np.squeeze(output['output_3'])\n",
        "\n",
        "  results = []\n",
        "  for i in range(count):\n",
        "    if scores[i] >= threshold:\n",
        "      result = {\n",
        "        'bounding_box': boxes[i],\n",
        "        'class_id': classes[i],\n",
        "        'score': scores[i]\n",
        "      }\n",
        "      results.append(result)\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "m68Xs6DYocVO"
      },
      "outputs": [],
      "source": [
        "def run_odt_and_draw_results(image_path, interpreter, threshold=0.5):\n",
        "  ''' Run object detection on the input image and draw the detection results\n",
        "  '''\n",
        "  \n",
        "  # Load the input shape required by the model\n",
        "  _, input_height, input_width, _ = interpreter.get_input_details()[0]['shape']\n",
        "\n",
        "  # Load the input image and preprocess it\n",
        "  preprocessed_image, original_image = preprocess_image(\n",
        "      image_path,\n",
        "      (input_height, input_width)\n",
        "    )\n",
        "\n",
        "  # Run object detection on the input image\n",
        "  results = detect_objects(interpreter, preprocessed_image, threshold=threshold)\n",
        "\n",
        "  # Plot the detection results on the input image\n",
        "  original_image_np = original_image.numpy().astype(np.uint8)\n",
        "  for obj in results:\n",
        "    # Convert the object bounding box from relative coordinates to absolute\n",
        "    # coordinates based on the original image resolution\n",
        "    ymin, xmin, ymax, xmax = obj['bounding_box']\n",
        "    xmin = int(xmin * original_image_np.shape[1])\n",
        "    xmax = int(xmax * original_image_np.shape[1])\n",
        "    ymin = int(ymin * original_image_np.shape[0])\n",
        "    ymax = int(ymax * original_image_np.shape[0])\n",
        "\n",
        "    # Find the class index of the current object\n",
        "    class_id = int(obj['class_id'])\n",
        "\n",
        "    # Draw the bounding box and label on the image\n",
        "    color = [int(c) for c in COLORS[class_id]]\n",
        "    cv2.rectangle(original_image_np, (xmin, ymin), (xmax, ymax), color, 2)\n",
        "    # Make adjustments to make the label visible for all objects\n",
        "    y = ymin - 15 if ymin - 15 > 15 else ymin + 15\n",
        "    label = \"{}: {:.0f}%\".format(CLASSES[class_id], obj['score'] * 100)\n",
        "    cv2.putText(original_image_np, label, (xmin, y),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "  # Return the final image\n",
        "  original_uint8 = original_image_np.astype(np.uint8)\n",
        "  return original_uint8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N3z-QDrooH9"
      },
      "source": [
        "## Cargar Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMWHqmwEoonk"
      },
      "source": [
        "### Cargo los datos de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "InmLpA7UouAg"
      },
      "outputs": [],
      "source": [
        "TRAIN_PATH = 'IBODS_LBL-1/train'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "V-Z4DiDIowbE"
      },
      "outputs": [],
      "source": [
        "train_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    TRAIN_PATH,\n",
        "    TRAIN_PATH,\n",
        "    labels\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we25ghDBoz4k"
      },
      "source": [
        "### Cargo los datos de validacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "dKrF-Fbdo0cn"
      },
      "outputs": [],
      "source": [
        "VALID_PATH = 'IBODS_LBL-1/valid'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EVpeegbdo2sf"
      },
      "outputs": [],
      "source": [
        "valid_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    VALID_PATH,\n",
        "    VALID_PATH,\n",
        "    labels\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf_R0LmKo7tH"
      },
      "source": [
        "### Cargo los datos de testeo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wub7vz2Mo95l"
      },
      "outputs": [],
      "source": [
        "TEST_PATH = 'IBODS_LBL-1/test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rmNJlBOoo-l4"
      },
      "outputs": [],
      "source": [
        "test_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    TEST_PATH,\n",
        "    TEST_PATH,\n",
        "    labels\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfmDFT7WpObT"
      },
      "source": [
        "## Entrenamiento de la IA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LV1ms_3pO-F"
      },
      "outputs": [],
      "source": [
        "model = object_detector.create(\n",
        "    train_data,\n",
        "    model_spec= 'efficientdet_lite1',\n",
        "    batch_size= 8,\n",
        "    train_whole_model=True,\n",
        "    epochs=100,\n",
        "    validation_data=valid_data\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnZ4nw2HpRHo"
      },
      "source": [
        "## Testeo de la IA y su performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hmk8-aIgpTjS"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLiSt_KKpY96"
      },
      "source": [
        "## Salvo el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "oULQdRCKpd8-"
      },
      "outputs": [],
      "source": [
        "os.mkdir('logs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYi2BYIipiab"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='logs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZLF_VFDploi"
      },
      "outputs": [],
      "source": [
        "model.evaluate_tflite('logs/model.tflite', test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LJza3uUpoOo"
      },
      "source": [
        "## Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "VsgwnBNSpo1d"
      },
      "outputs": [],
      "source": [
        "config = QuantizationConfig.for_float16()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "7SHpEPMepsXy",
        "outputId": "112fb287-adcf-4a22-e786-fed81e3049a1"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-fac64049725a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logs/model1.tflite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupported_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtflite_quant_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mfrom_saved_model\u001b[0;34m(cls, saved_model_dir, signature_keys, tags)\u001b[0m\n\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1648\u001b[0;31m       \u001b[0msaved_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_model_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1649\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignature_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m       \u001b[0msignature_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(export_dir, tags, options)\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdon\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0ma\u001b[0m \u001b[0mMetaGraph\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mSavedModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m   \"\"\"\n\u001b[0;32m--> 936\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"root\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[1;32m    947\u001b[0m     \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m   saved_model_proto, debug_info = (\n\u001b[0;32m--> 949\u001b[0;31m       loader_impl.parse_saved_model_with_debug_info(export_dir))\n\u001b[0m\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m   if (len(saved_model_proto.meta_graphs) == 1 and\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model_with_debug_info\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mMissing\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0minfo\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mfine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m   \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m   \u001b[0msaved_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m   debug_info_path = file_io.join(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    114\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     raise IOError(\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;34mf\"SavedModel file does not exist at: {export_dir}{os.path.sep}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;34mf\"{{{constants.SAVED_MODEL_FILENAME_PBTXT}|\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         f\"{constants.SAVED_MODEL_FILENAME_PB}}}\")\n",
            "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: logs/model1.tflite/{saved_model.pbtxt|saved_model.pb}"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model('logs/model1.tflite')\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "tflite_quant_model = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgUNYM4Pps7l"
      },
      "source": [
        "## Deteccion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gEnhMAMpy4k"
      },
      "source": [
        "### Deteccion **simple** fotos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrOyAoyYqOHG"
      },
      "outputs": [],
      "source": [
        "detector = vision.ObjectDetector.create_from_file('logs/model1.tflite')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gR4vBgxdqQOM"
      },
      "outputs": [],
      "source": [
        "tensor_image = vision.TensorImage.create_from_file('IBODS_LBL-1/test/Audi_A7_2012_59_18_310_30_6_75_55_195_18_AWD_4_4_4dr_aqB_jpg.rf.2cdf87614d0d1c937d1e97bbdd338b2c.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neMJCmkCqSTH"
      },
      "outputs": [],
      "source": [
        "detector.detect(tensor_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE4m-KPaqWRM"
      },
      "source": [
        "### Deteccion **compleja** fotos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRkGBtB0qWzB"
      },
      "outputs": [],
      "source": [
        "cwd = os.getcwd()\n",
        "os.mkdir(f'{cwd}/result')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ_pluR7qbXc"
      },
      "outputs": [],
      "source": [
        "cwd = os.getcwd()\n",
        "\n",
        "DETECTION_THRESHOLD = 0.3\n",
        "\n",
        "# Change the test file path to your test image\n",
        "INPUT_IMAGE_PATH = 'IBODS_LBL-1/test/Audi_A7_2012_59_18_310_30_6_75_55_195_18_AWD_4_4_4dr_aqB_jpg.rf.2cdf87614d0d1c937d1e97bbdd338b2c.jpg'\n",
        "\n",
        "im = Image.open(INPUT_IMAGE_PATH)\n",
        "im.thumbnail((512, 512), Image.ANTIALIAS)\n",
        "im.save(f'{cwd}/result/input.png','PNG')\n",
        "\n",
        "# Load the TFLite model\n",
        "model_path = 'logs/model1.tflite'\n",
        "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Run inference and draw detection result on the local copy of the original file\n",
        "detection_result_image = run_odt_and_draw_results(\n",
        "    INPUT_IMAGE_PATH,\n",
        "    interpreter,\n",
        "    threshold=DETECTION_THRESHOLD\n",
        ")\n",
        "\n",
        "# Show the detection result\n",
        "img = Image.fromarray(detection_result_image)\n",
        "img.save(f'{cwd}/result/ouput.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIEa4jO8qftk"
      },
      "source": [
        "### Deteccion en video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDsFyKYHqgPi"
      },
      "outputs": [],
      "source": [
        "detector = vision.ObjectDetector.create_from_file('logs/model1.tflite')\n",
        "\n",
        "import cv2\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(\"No se puede abrir la camara\")\n",
        "    exit()\n",
        "\n",
        "while True:\n",
        "  ret,frame = cap.read() # Voy agarrando los frames\n",
        "\n",
        "  if ret == True: # Si leyo correctamente los frames, signfica que el retun (ret) es true, por lo que ejecuto el codigo en ese caso\n",
        "    cv2.cvtColor(frame, cv2.COLOR_BGRRGB) # Convierto todos los frames al espectro RGB\n",
        "    tensor_image = vision.TensorImage.create_from_array(frame) # Convierto todos los frames a tensores\n",
        "    detector.detect(tensor_image)\n",
        "    key = cv2.waitKey(1)\n",
        "    if key == ord('q'):\n",
        "      break\n",
        "  if not ret:\n",
        "        print(\"No puedo recibir imagenes (finalizaste la trasmision?). Saliendo ...\")\n",
        "        break\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMU8XliBrjr/4gzqsbr4eI0",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.10 ('IBODS')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "70597fcbe4b30bd6fc91e4917cda1902a5a52d70ec888f4770bd319ddb0fce49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
